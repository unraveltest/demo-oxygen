<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE task
  PUBLIC "-//OASIS//DTD DITA Task//EN" "task.dtd" ><task xml:lang="en-us" id="Installing-Unravel-Sensor-for-Individual-Applications-Submitted-Through-spark-shell_53908545">
   <title>
                        Installing Unravel Sensor for Individual Applications
                            Submitted Through spark-shell 
                    </title>
   <taskbody>
      <context>
         <p>Introduction</p>
         <p>This topic explains how to set up Unravel Sensor for Spark to profile <b>specific Spark applications</b>
            executed via the <b>spark-shell</b> (in other words, <i>per-application profiling</i>, also called "dev
            mode"), rather than <i>cluster-wide profiling</i>. The Unravel Sensor for Spark uses Java agents to
            intercept Spark applications at runtime. Four JARs are included in the Unravel Sensor .zip package:
               <codeph>btrace-agent.jar</codeph>, <codeph>btrace-boot.jar</codeph>, <codeph>unravel-boot.jar, and
               unravel-sys.jar</codeph>. The JARs are fired up when you define
               <codeph>spark.driver.extraJavaOptions</codeph> and <codeph>spark.executor.extraJavaOptions</codeph> as
            part of your <codeph>spark-submit</codeph> command.</p>
         <p>The information here applies to Spark versions 1.3.x through 2.0.x. <ph>Brown</ph> text indicates where you
            must substitute your particular values.</p>
         <p>1. Obtain the Sensor</p>
         <p>The Unravel Sensor is included in the Unravel Server RPM installation. After installing the Unravel Server
            RPM on <ph>
               <codeph>UNRAVEL_HOST</codeph>
            </ph>, you can obtain the sensor either from the filesystem on the Unravel Server host, or by HTTP from <ph
               outputclass="nolink">
               <codeph>http://<ph>UNRAVEL_HOST</ph>:3000/hh/spark-<ph>VERSION</ph>/unravel-sensor-for-spark-bin.zip</codeph>.</ph>
         </p>
         <p>
            <ph outputclass="nolink">To obtain Unravel Sensor from the filesystem on the Unravel Server host:</ph>
         </p>
         <ul>
            <li>Go to the <codeph>/usr/local/unravel/webapps/ROOT/hh/spark-<ph>VERSION</ph>/</codeph> directory on the
               Unravel Server host, where <ph>
                  <codeph>VERSION</codeph>
               </ph> is<ul>
                  <li>
                     <codeph>2.0</codeph> for Spark 2.0.x</li>
                  <li>
                     <codeph>1.6</codeph> for Spark 1.6.x</li>
                  <li>
                     <codeph>1.5</codeph> for Spark 1.5.x</li>
                  <li>
                     <codeph>1.3</codeph> for Spark 1.3.x</li>
               </ul>
            </li>
            <li>Within this directory, locate the sensor file: <codeph>unravel-sensor-for-spark-bin.zip</codeph>.</li>
         </ul>
         <p>2. Run the Sensor to Intercept Spark Apps executed via the spark-shell</p>
         <p outputclass="p1">To intercept Spark apps executed via the spark-shell, <ph>you need to unzip the Unravel
               Sensor .zip file on the client node at a location on the local file system that is readable by all users,
               referred to as </ph>
            <ph>
               <codeph>UNZIPPED_ARCHIVE_DEST</codeph>
            </ph>
            <ph> below. We suggest </ph>
            <codeph>/usr/local/unravel-spark</codeph>
            <ph>.</ph>
         </p>
         <p>If you use multiple hosts as clients, do this step for <b>each client</b>.</p>
         <codeblock outputclass="syntaxhighlighter-pre">mkdir $UNZIPPED_ARCHIVE_DEST
cd $UNZIPPED_ARCHIVE_DEST 
wget http://$UNRAVEL_HOST:3000/hh/spark-VERSION/unravel-sensor-for-spark-bin.zip
unzip unravel-sensor-for-spark-bin.zip</codeblock>
         <p outputclass="title">Important</p>
         <p>Please keep the original <codeph>unravel-sensor-for-spark-bin.zip</codeph> file inside <ph>
               <codeph>UNZIPPED_ARCHIVE_DEST</codeph>
            </ph>.</p>
         <p>Define <codeph>spark.driver.extraJavaOptions</codeph> and <codeph>spark.executor.extraJavaOptions</codeph>
            as part of your <codeph>spark-shell</codeph> command. Please note that spark-shell is <u>always executed in
               client mode</u>, so a deployment mode setting is not available. </p>
         <ph outputclass="aui-icon aui-icon-small aui-iconfont-approve confluence-information-macro-icon"/>
         <p>To use the example below, be sure to replace <ph>
               <codeph>UNRAVEL_SENSOR_PATH</codeph>
            </ph>, <ph>
               <codeph>UNRAVEL_SERVER_IP_PORT</codeph>
            </ph>, <ph>
               <codeph>SPARK_EVENT_LOG_DIR</codeph>
            </ph>, and <ph>
               <codeph>PATH_TO_SPARK_EXAMPLE_JAR</codeph>
            </ph> with your values.</p>
         <ul>
            <li>
               <codeph>
                  <ph>UNRAVEL_SENSOR_PATH</ph>
               </codeph>: Parent directory of the Unravel Sensor .zip file, <codeph>
                  <ph>unravel-sensor-for-spark-bin.zip</ph>
               </codeph>. If you put this file on HDFS, <codeph>
                  <ph>UNRAVEL_SENSOR_PATH</ph>
               </codeph> is the parent directory on HDFS.</li>
            <li>
               <codeph>
                  <ph>UNRAVEL_SERVER_IP_PORT</ph>
               </codeph>: IP address and port of the <codeph>unravel_lr</codeph> service in the format
                  <codeph>IP:PORT</codeph>. Port is 4043 by default. Sample value:
               <codeph>10.0.0.142:4043</codeph>.</li>
            <li>
               <ph>
                  <codeph>SPARK_EVENT_LOG_DIR</codeph>
               </ph>: Location of the event log directory on HDFS, S3, or local file system. If a remote address is
               used, include the namenode IP address and port.</li>
         </ul>
         <ul>
            <li>
               <codeph>
                  <ph>UNZIPPED_ARCHIVE_DEST</ph>
               </codeph>: Directory on the local file system that contains the unzipped content of
                  <codeph>unravel-sensor-for-spark-bin.zip</codeph>. It also contains the original
                  <codeph>unravel-sensor-for-spark-bin.zip</codeph> file.</li>
         </ul>
         <p/>
         <p>For example,</p>
         <codeblock outputclass="syntaxhighlighter-pre">export UNZIPPED_ARCHIVE_DEST=$UNZIPPED_ARCHIVE_DEST
export UNRAVEL_SERVER_IP_PORT=$UNRAVEL_SERVER_IP_PORT
export SPARK_EVENT_LOG_DIR=$SPARK_EVENT_LOG_DIR

export ENABLED_SENSOR_FOR_DRIVER="spark.driver.extraJavaOptions=-javaagent:$UNZIPPED_ARCHIVE_DEST/btrace-agent.jar=bootClassPath=$UNZIPPED_ARCHIVE_DEST/unravel-boot.jar,systemClassPath=$UNZIPPED_ARCHIVE_DEST/unravel-sys.jar,scriptOutputFile=/dev/null,script=DriverProbe.class:SQLProbe.class -Dcom.sun.btrace.FileClient.flush=-1 -Dcom.unraveldata.spark.sensor.disableLiveUpdates=true"


export ENABLED_SENSOR_FOR_EXECUTOR="spark.executor.extraJavaOptions=-javaagent:unravel-sensor-for-spark-bin.zip/btrace-agent.jar=bootClassPath=unravel-sensor-for-spark-bin.zip/unravel-boot.jar,systemClassPath=unravel-sensor-for-spark-bin.zip/unravel-sys.jar,scriptOutputFile=/dev/null,script=ExecutorProbe.class  -Dcom.sun.btrace.FileClient.flush=-1"



/usr/lib/spark/bin/spark-shell \
    --archives $UNZIPPED_ARCHIVE_DEST/unravel-sensor-for-spark-bin.zip \
    --conf "$ENABLED_SENSOR_FOR_DRIVER" \
    --conf "$ENABLED_SENSOR_FOR_EXECUTOR" \
    --conf "spark.unravel.server.hostport=$UNRAVEL_SERVER_IP_PORT" \
    --conf "spark.eventLog.dir=${SPARK_EVENT_LOG_DIR}" \
    --conf "spark.eventLog.enabled=true" \
&lt;&lt;EOF
// your spark-shell code snippet will follow here
// For exemplifying, we use a snippet of RDDRelation below

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.functions._

import sqlContext.implicits._

case class Record(key: Int, value: String)

    val df = sc.parallelize((1 to 100).map(i =&gt; Record(i, s"val_$i"))).toDF()

    // Any RDD containing case classes can be registered as a table.  The schema of the table is
    // automatically inferred using scala reflection.
    df.registerTempTable("records")

    // Once tables have been registered, you can run SQL queries over them.
    println("Result of SELECT *:")
    sqlContext.sql("SELECT * FROM records").collect().foreach(println)

    // Aggregation queries are also supported.
    val count = sqlContext.sql("SELECT COUNT(*) FROM records").collect().head.getLong(0)
    println(s"COUNT(*): $count")

exit

EOF

</codeblock>
         <note>
            <p> Note that a full blank line separates lengthy lines that are wrapped, except for the spark-shell that
               uses line continuation backslashes. </p>
         </note>
      </context>
   </taskbody>
</task>
